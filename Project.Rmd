---
title: "TextMiningProject"
output: html_document
---

```{r}
require("jsonlite")
require("request")
require("tidyverse")
require("stringr")
```


```{r}
require(pdflatex)
require(tinytex)
require(quanteda)
require(quanteda.textmodels)
require(quanteda.textstats)
require(quanteda.textplots)
# devtools::install_github("kbenoit/quanteda.dictionaries") 
require(quanteda.dictionaries)
require(readtext)
require(devtools)
require(quanteda.corpora)
require(newsmap)
require(seededlda)
require(ggplot2)
```

```{r}
subreddit = "worldnews"
timepoint = 1576399066
```



```{r}
getTopPosts = function(before,after){
  
  base_url = "https://api.pushshift.io/reddit/search/"
  submission_endpoint = "submission/"
  sort = "desc"
  entries = 50
  fields=paste("author","created_utc","domain","full_link","num_comments","score","title","id",sep=",")
  # so that we get the top posts
  sort_type = "score"
  
  
  
  query_top_posts = paste(base_url, submission_endpoint,"?",
                        "subreddit=",subreddit,"&",
                        "sort=",sort,"&",
                        "size=",entries,"&",
                        "before=",before,"&",
                        "after=",after,"&",
                        "sort_type=",sort_type,"&",
                        "fields=",fields,
                        sep="")
  q1 = fromJSON(query_top_posts)
  return(as.matrix(q1$data))

}

#top_posts = getTopPosts()
```





```{r}


getComments = function(comment){
  df = subset(comment,select = c("id","author","ups","body","created_utc"))
  
  for (repl in comment[["replies"]]) {
    if(!is.null(repl) && !is.character(repl) && !is.null(repl$data$children)){
      for(i in 1:length(repl$data$children)){
        elem = as.data.frame(repl$data$children[i])$data
        if(!is.null(elem)){
            df = rbind(df, getComments(elem))
        }
      }
    }
  }
  return(df)
}


getAllCommentsById = function(id){
  base_url = "https://www.reddit.com/r/"
  comment_endpoint = "comments/"

  
  sort = "top" #alternative: controversial
  entries = 20
  depth = 3
  
  query_comments = paste(base_url,subreddit,"/", comment_endpoint,id,".json?",
                        "depth=",depth,"&",
                        "limit=",entries,"&",
                        "showtitle=false&",
                        "showedits=false&",
                        "showmedia=false&",
                        "showmore=false&",
                        "sort=",sort,
                        sep="")
  commentFromJson = fromJSON(query_comments)
  query_comments
  raw_df = as.data.frame(commentFromJson$data$children[2])$data

  df = getComments(comment = raw_df)
  df = df[df$body != "[deleted]",]
  df = df[df$body != "[removed]",]
  return(df)
}
id = "aengo3"
df_result = getAllCommentsById(id)
df_result
```

```{r}
yearvector = c("2013","2014","2015","2016","2017")

for(onlyyear in yearvector){
  
baseyear =  as.Date(paste(onlyyear,"-01-01",sep = ""))
year =  baseyear
dateToEpoch = function(d){
  return(as.integer(as.POSIXct(d)))
}

topPosts2019 = data.frame()



for (i in seq(2,52,2)) {
  after = dateToEpoch(year)
  before = dateToEpoch(year + 7 * i) - 1
  tp = getTopPosts(before = before,after = after)
  tp = as.data.frame(tp)
  topPosts2019 = rbind(topPosts2019,tp)
  print(year)
  year = baseyear + 7 * i
  Sys.sleep(5)
}

#more than 10 comments
typeof(topPosts2019$num_comments)

topPosts2019 = topPosts2019[as.integer(topPosts2019$num_comments) > 10,]
topPosts2019 = unique(topPosts2019)


saveRDS(topPosts2019, file = paste("topPostsEvery2Weeks",onlyyear,".rds",sep=""))


allComments2019ForPosts = data.frame()
for(i in 1:nrow(topPosts2019)){
  row = topPosts2019[i,]
  print(row$id)
  comments = getAllCommentsById(row$id)
  if(nrow(comments) != 0){
    comments = cbind(comments, postId=row["id"])
    allComments2019ForPosts = rbind(allComments2019ForPosts,comments)
  }
}
names(allComments2019ForPosts)[6] <- "postId"

saveRDS(allComments2019ForPosts, file = paste("comments",onlyyear,".rds",sep=""))
}

```


```{r}
sources = aggregate(q2$domain,by=list(q2$domain), FUN=length) 
sources = as.data.frame(sources)
sources = filter(sources, x>1)



barplot(height=sources$x,names = sources$Group.1,
        xlab="posts", 
        ylab="news outlets", 
        main="Sources", 
        horiz=T, las=1,
        )
```


```{r}
rds2019 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2019.rds")
rds2020 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2020.rds")
rds2021 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2020.rds")

rds2019_2021 = rbind(rds2019,rds2020,rds2021)
toks19_21 = tokens(corpus(rds2019_2021, text_field= "title"), remove_punct = TRUE) %>% tokens_remove(pattern = stopwords("en"))
dfm19_21 = dfm(toks19_21)
covid_theme = c("covid","coronavirus","covid-19")
covid_simil <- textstat_simil(dfm19_21,dfm19_21[,covid_theme], method = "correlation", margin = "features",min_simil = 0.3)
covid_simil = as.data.frame(covid_simil)
words = as.vector(covid_simil$feature1)
words = c(words,covid_theme)
words = c(words,"corona")
greplstring = paste(words,sep="|",collapse="|")
greplstring

covid_data_frame = rds2019_2021 %>%  filter(grepl(greplstring,title))

rds2016 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2016.rds")
rds2017 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2017.rds")
rds2018 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2018.rds")
rds2019 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2019.rds")
rds2020 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2020.rds")


rds2016_2020 = rbind(rds2016,rds2017,rds2018,rds2019,rds2020)
toks16_20 = tokens(corpus(rds2016_2020, text_field= "title"), remove_punct = TRUE) %>% tokens_remove(pattern = stopwords("en"))
dfm16_20 = dfm(toks16_20)



trump_theme = c("trump","donald","president")
trump_simil <- textstat_simil(dfm16_20,dfm16_20[,trump_theme], method = "correlation", margin = "features",min_simil = 0.3)
trump_simil = as.data.frame(trump_simil)
words = as.vector(trump_simil$feature1)
words = c(words,trump_theme)
words = c(words)
greplstring = paste(words,sep="|",collapse="|")
greplstring
trump_data_frame = rds2019_2021 %>%  filter(grepl(greplstring,title))


rds2013 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2013.rds")
rds2014 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2014.rds")
rds2015 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2015.rds")
rds2016 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2016.rds")
rds2017 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2017.rds")
rds2018 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2018.rds")
rds2019 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2019.rds")
rds2020 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2020.rds")
rds2021 = readRDS("C:\\Users\\kerbe\\Desktop\\TextMining\\project\\TextMiningProject\\data\\topPostsEvery2Weeks2021.rds")
wholerds = rbind(rds2013,rds2014,rds2015,rds2016,rds2017,rds2018,rds2019,rds2020,rds2021)
wholetok = tokens(corpus(wholerds, text_field= "title"), remove_punct = TRUE) %>% tokens_remove(pattern = stopwords("en"))
wholedfm = dfm(wholetok)

europe_theme = c("europe","eu")
europe_simil <- textstat_simil(wholedfm,wholedfm[,europe_theme], method = "correlation", margin = "features",min_simil = 0.3)
europe_simil = as.data.frame(europe_simil)
words = as.vector(europe_simil$feature1)
words = c(words,europe_theme)
words = c(words)
words = europe_theme
greplstring = paste(words,sep="|",collapse="|")
greplstring
europe_data_frame = wholerds %>%  filter(grepl(greplstring,title))


russia_theme = c("russia","putin","vladimir")
russia_simil <- textstat_simil(wholedfm,wholedfm[,russia_theme], method = "correlation", margin = "features",min_simil = 0.3)
russia_simil = as.data.frame(russia_simil)
words = as.vector(russia_simil$feature1)
words = c(words,russia_theme)
words = c(words)
greplstring = paste(words,sep="|",collapse="|")
greplstring
russia_data_frame = wholerds %>%  filter(grepl(greplstring,title))

saveRDS(object = russia_data_frame, "russia.rds")
saveRDS(object = trump_data_frame, "trump.rds")
saveRDS(object = covid_data_frame, "covid.rds")

```


